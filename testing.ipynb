{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pettingzoo.test import api_test\n",
    "import pettingzoo\n",
    "import gymnasium as gym\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from lib.briscola_env.briscola_env import BriscolaEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API test\n",
      "Passed API test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:242: UserWarning: Observation space for each agent probably should be gymnasium.spaces.box or gymnasium.spaces.discrete\n",
      "  warnings.warn(\n",
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:140: UserWarning: Observation is not a NumPy array\n",
      "  warnings.warn(\"Observation is not a NumPy array\")\n",
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:660: UserWarning: Environment has not defined a render() method\n",
      "  warnings.warn(\"Environment has not defined a render() method\")\n"
     ]
    }
   ],
   "source": [
    "env = BriscolaEnv()\n",
    "api_test(env, num_cycles=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pass into other gymnasium wrappers, we need to ensure that pettingzoo's wrappper\n",
    "# can also be a gymnasium Env. Thus, we subclass under gym.Env as well.\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper, gym.Env):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\n",
    "\n",
    "        The observation is for the next agent (used to determine the next action), while the remaining\n",
    "        items are for the agent that just acted (used to understand what just happened).\n",
    "        \"\"\"\n",
    "        current_agent = self.agent_selection\n",
    "\n",
    "        super().step(action)\n",
    "\n",
    "        next_agent = self.agent_selection\n",
    "        return (\n",
    "            self.observe(next_agent),\n",
    "            self._cumulative_rewards[current_agent],\n",
    "            self.terminations[current_agent],\n",
    "            self.truncations[current_agent],\n",
    "            self.infos[current_agent],\n",
    "        )\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    return env.action_mask()\n",
    "\n",
    "def train(\n",
    "    steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = BriscolaEnv()\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "    env.reset(seed=seed)\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata)}.\")\n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps)\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(steps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_action_mask(player, num_games=100):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = BriscolaEnv()\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs random agents. Trained agent will play as {env.possible_agents[player]}.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "    print(\"using\", latest_policy)\n",
    "    model = MaskablePPO.load(latest_policy)\n",
    "\n",
    "    scores = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                winner = max(env.rewards, key=env.rewards.get)\n",
    "                scores[winner] += env.rewards[\n",
    "                    winner\n",
    "                ]  # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                break\n",
    "            else:\n",
    "                if agent != env.possible_agents[player]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    print(\"Winrates:\")\n",
    "    for p in env.possible_agents:\n",
    "        if sum(scores.values()) == 0:\n",
    "            winrate = 0\n",
    "        else:\n",
    "            winrate = scores[p] / sum(scores.values())\n",
    "        print(f\"\\t{p}: {winrate*100}%`\")\n",
    "        print(f\"\\t{total_rewards[p]}\")\n",
    "    print(\"Total rewards: \", total_rewards)\n",
    "    print(\"Final scores: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- At position 0 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_0.\n",
      "using briscola_20250427-142816.zip\n",
      "Winrates:\n",
      "\tplayer_0: 32.322438717787556%`\n",
      "\t19939\n",
      "\tplayer_1: 16.905641106222504%`\n",
      "\t11726\n",
      "\tplayer_2: 30.768777498428662%`\n",
      "\t18708\n",
      "\tplayer_3: 20.003142677561282%`\n",
      "\t12786\n",
      "Total rewards:  {'player_0': 19939, 'player_1': 11726, 'player_2': 18708, 'player_3': 12786}\n",
      "Final scores:  {'player_0': 16456, 'player_1': 8607, 'player_2': 15665, 'player_3': 10184}\n",
      "--- At position 1 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_1.\n",
      "using briscola_20250427-142816.zip\n",
      "Winrates:\n",
      "\tplayer_0: 28.867187500000004%`\n",
      "\t17753\n",
      "\tplayer_1: 21.064453125%`\n",
      "\t13870\n",
      "\tplayer_2: 33.09765625%`\n",
      "\t20363\n",
      "\tplayer_3: 16.970703125%`\n",
      "\t11796\n",
      "Total rewards:  {'player_0': 17753, 'player_1': 13870, 'player_2': 20363, 'player_3': 11796}\n",
      "Final scores:  {'player_0': 14780, 'player_1': 10785, 'player_2': 16946, 'player_3': 8689}\n",
      "--- At position 2 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_2.\n",
      "using briscola_20250427-142816.zip\n",
      "Winrates:\n",
      "\tplayer_0: 28.921780892020628%`\n",
      "\t17847\n",
      "\tplayer_1: 16.00992008817856%`\n",
      "\t11284\n",
      "\tplayer_2: 35.968192733141755%`\n",
      "\t21599\n",
      "\tplayer_3: 19.100106286659056%`\n",
      "\t12521\n",
      "Total rewards:  {'player_0': 17847, 'player_1': 11284, 'player_2': 21599, 'player_3': 12521}\n",
      "Final scores:  {'player_0': 14694, 'player_1': 8134, 'player_2': 18274, 'player_3': 9704}\n",
      "--- At position 3 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_3.\n",
      "using briscola_20250427-142816.zip\n",
      "Winrates:\n",
      "\tplayer_0: 34.87935184641578%`\n",
      "\t21295\n",
      "\tplayer_1: 16.01596900135032%`\n",
      "\t10433\n",
      "\tplayer_2: 24.076792109434628%`\n",
      "\t16213\n",
      "\tplayer_3: 25.027887042799275%`\n",
      "\t15852\n",
      "Total rewards:  {'player_0': 21295, 'player_1': 10433, 'player_2': 16213, 'player_3': 15852}\n",
      "Final scores:  {'player_0': 17823, 'player_1': 8184, 'player_2': 12303, 'player_3': 12789}\n"
     ]
    }
   ],
   "source": [
    "for position in range(4):\n",
    "\tprint(f\"--- Testing position {position} ---\")\n",
    "\teval_action_mask(position, num_games=1_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
