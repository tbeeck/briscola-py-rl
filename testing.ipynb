{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pettingzoo.test import api_test\n",
    "import pettingzoo\n",
    "import gymnasium as gym\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from lib.briscola_env.briscola_env import BriscolaEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API test\n",
      "Passed API test\n"
     ]
    }
   ],
   "source": [
    "env = BriscolaEnv()\n",
    "api_test(env, num_cycles=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pass into other gymnasium wrappers, we need to ensure that pettingzoo's wrappper\n",
    "# can also be a gymnasium Env. Thus, we subclass under gym.Env as well.\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper, gym.Env):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\n",
    "\n",
    "        The observation is for the next agent (used to determine the next action), while the remaining\n",
    "        items are for the agent that just acted (used to understand what just happened).\n",
    "        \"\"\"\n",
    "        current_agent = self.agent_selection\n",
    "\n",
    "        super().step(action)\n",
    "\n",
    "        next_agent = self.agent_selection\n",
    "        return (\n",
    "            self.observe(next_agent),\n",
    "            self._cumulative_rewards[current_agent],\n",
    "            self.terminations[current_agent],\n",
    "            self.truncations[current_agent],\n",
    "            self.infos[current_agent],\n",
    "        )\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    return env.action_mask()\n",
    "\n",
    "def train(\n",
    "    steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = BriscolaEnv()\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "    env.reset(seed=seed)\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata)}.\")\n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps)\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on {'render_modes': []}.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 40       |\n",
      "|    ep_rew_mean     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 294      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 40            |\n",
      "|    ep_rew_mean          | 1.67e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 243           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 16            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028958335 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.949        |\n",
      "|    explained_variance   | -0.000286     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.81e+05      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00385      |\n",
      "|    value_loss           | 4.94e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 40           |\n",
      "|    ep_rew_mean          | 1.92e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 226          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003539953 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.948       |\n",
      "|    explained_variance   | 0.000136     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.46e+05     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00386     |\n",
      "|    value_loss           | 7.93e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 40            |\n",
      "|    ep_rew_mean          | 1.88e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 217           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 37            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032921473 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.948        |\n",
      "|    explained_variance   | 3.68e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.95e+05      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00365      |\n",
      "|    value_loss           | 1e+06         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 40            |\n",
      "|    ep_rew_mean          | 1.75e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 211           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 48            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042565019 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.947        |\n",
      "|    explained_variance   | 1.91e-06      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.77e+05      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00438      |\n",
      "|    value_loss           | 7.65e+05      |\n",
      "-------------------------------------------\n",
      "Model has been saved.\n",
      "Finished training on briscola.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_action_mask(player, num_games=100):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = BriscolaEnv()\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs a random agent. Trained agent will play as {env.possible_agents[player]}.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "    print(\"using\", latest_policy)\n",
    "    model = MaskablePPO.load(latest_policy)\n",
    "\n",
    "    scores = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                winner = max(env.rewards, key=env.rewards.get)\n",
    "                scores[winner] += env.rewards[\n",
    "                    winner\n",
    "                ]  # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                break\n",
    "            else:\n",
    "                if agent != env.possible_agents[player]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    print(\"Winrates:\")\n",
    "    for p in env.possible_agents:\n",
    "        if sum(scores.values()) == 0:\n",
    "            winrate = 0\n",
    "        else:\n",
    "            winrate = scores[p] / sum(scores.values())\n",
    "        print(f\"\\t{p}: {winrate}\")\n",
    "        print(f\"\\t{total_rewards[p]}\")\n",
    "    print(\"Total rewards: \", total_rewards)\n",
    "    print(\"Final scores: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation vs a random agent. Trained agent will play as player_1.\n",
      "using briscola_20250426-232304.zip\n",
      "Winrates:\n",
      "\tplayer_0: 0.3187653592843802\n",
      "\t19692\n",
      "\tplayer_1: 0.19136931092106557\n",
      "\t12501\n",
      "\tplayer_2: 0.2887840361741866\n",
      "\t18173\n",
      "\tplayer_3: 0.20108129362036764\n",
      "\t13175\n",
      "Total rewards:  {'player_0': 19692, 'player_1': 12501, 'player_2': 18173, 'player_3': 13175}\n",
      "Final scores:  {'player_0': 16214, 'player_1': 9734, 'player_2': 14689, 'player_3': 10228}\n"
     ]
    }
   ],
   "source": [
    "eval_action_mask(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
