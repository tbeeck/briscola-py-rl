{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "import onnx\n",
    "import torch as th\n",
    "from pettingzoo.test import api_test\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, Logger\n",
    "\n",
    "from lib.action_mask_wrapper import SB3ActionMaskWrapper\n",
    "from lib.briscola.game import BriscolaGame\n",
    "from lib.briscola_env.briscola_env import BriscolaEnv\n",
    "from lib.briscola_env.embedding import game_embedding\n",
    "from lib.mlflow_logging import MLflowOutputFormat\n",
    "from lib.onnxable import OnnxableMaskableACPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_COUNT = 4\n",
    "NUM_TRAINING_STEPS = 150_000\n",
    "NUM_EVAL_GAMES = 500\n",
    "\n",
    "MODEL_HYPERPARAMS = {\n",
    "\t\"n_steps\": 40 * 4,\n",
    "\t\"batch_size\": 40,\n",
    "\t\"learning_rate\": 3e-4,\n",
    "\t\"ent_coef\": 0.01,\n",
    "\t\"vf_coef\": 0.65,\n",
    "\t\"normalize_advantage\": True,\n",
    "}\n",
    "\n",
    "def make_env():\n",
    "\treturn BriscolaEnv(num_players=PLAYER_COUNT)\n",
    "\n",
    "def make_model(env):\n",
    "\treturn MaskablePPO(\n",
    "        MaskableActorCriticPolicy, \n",
    "        env, \n",
    "        verbose=1,\n",
    "        **MODEL_HYPERPARAMS,\n",
    "    )\n",
    "\n",
    "api_test(make_env(), num_cycles=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(\n",
    "    steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = SB3ActionMaskWrapper(make_env())\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    def mask_fn(env):\n",
    "        return env.action_mask()\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "\n",
    "    loggers = [HumanOutputFormat(sys.stdout), MLflowOutputFormat()]\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata)}.\")\n",
    "    model = make_model(env)\n",
    "    model.set_random_seed(seed)\n",
    "    model.set_logger(Logger(folder=None, output_formats=loggers))\n",
    "    model.learn(total_timesteps=steps)\n",
    "    model_path = f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "    model.save(model_path)\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "    env.close()\n",
    "    return model, model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Code\n",
    "def make_results(agents, wins, total_rewards):\n",
    "    player_results = []\n",
    "    total_wins = sum(wins.values())\n",
    "    print(\"Winrates:\")\n",
    "    for p in agents:\n",
    "        winrate = wins[p] / total_wins\n",
    "        print(f\"\\t{p}: {winrate*100}%\")\n",
    "        print(f\"\\tWins: {wins[p]} Rewards: {total_rewards[p]}\")\n",
    "        player_results.append({\"winrate\": winrate, \"total_rewards\": total_rewards[p]})\n",
    "    return player_results\n",
    "\n",
    "def eval_action_mask(player, model_path, num_games=100):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = make_env()\n",
    "    model = None\n",
    "    print(\"Starting evaluation vs random agents.\")\n",
    "    if model_path is not None:\n",
    "        print(f\"Trained agent will play as {env.possible_agents[player]}\")\n",
    "        print(\"Using policy:\", model_path)\n",
    "        model = MaskablePPO.load(model_path)\n",
    "\n",
    "    wins = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            observation, action_mask = obs.values()\n",
    "            if termination or truncation:\n",
    "                winner = max(env.rewards, key=env.rewards.get)\n",
    "                wins[winner] += env.infos[winner][\"wins\"] \n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                break\n",
    "            else:\n",
    "                # Act randomly unless it's the agents turn\n",
    "                if model is None or agent != env.possible_agents[player]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "    return make_results(env.agents, wins, total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Code\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def make_plot(winrate_data, title):\n",
    "\twinrates = np.transpose(np.array([\n",
    "\t\t[entry['winrate'] for entry in sublist] \n",
    "\t\tfor sublist in winrate_data\n",
    "   \t]))\n",
    "\tfig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\t# How many groups and bars\n",
    "\tnum_positions, num_players = winrates.shape\n",
    "\tx = np.arange(num_players)\n",
    "\n",
    "\t# Width of each bar group\n",
    "\tbar_width = 0.2\n",
    "\t\n",
    "\t# Plot each group's bars\n",
    "\tfor i in range(num_positions):\n",
    "\t\tax.bar(x + i * bar_width, winrates[i], width=bar_width, label=f'Player {i}')\n",
    "\n",
    "\t# Labels and stuff\n",
    "\tax.set_xlabel('Model Position')\n",
    "\tax.set_ylabel('Winrate')\n",
    "\tax.set_title(title)\n",
    "\tax.set_xticks(x + bar_width * (num_positions-1) / 2)\n",
    "\tax.set_xticklabels([str(i) for i in range(num_players)])\n",
    "\tax.legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\tplt.close(fig)\n",
    "\treturn fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Onnx\n",
    "def onnx_export(model_path):\n",
    "    model_source_file = model_path\n",
    "    model_dest_file = f\"{model_path}-probdist.onnx\"\n",
    "    model = MaskablePPO.load(model_source_file, device=\"cpu\")\n",
    "    onnx_policy = OnnxableMaskableACPolicy(model.policy)\n",
    "    observation_size = model.observation_space.shape\n",
    "    dummy_input = th.randn(1, *observation_size)\n",
    "    th.onnx.export(\n",
    "        onnx_policy,\n",
    "        dummy_input,\n",
    "        model_dest_file,\n",
    "        opset_version=17,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"dist\", \"values\"]\n",
    "    )\n",
    "    onnx_model = onnx.load(model_dest_file)\n",
    "    print(f\"Onnx model exported to {model_dest_file}\")\n",
    "    return onnx_model, model_dest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Track\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:8081\")\n",
    "mlflow.set_experiment(\"Briscola AI\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"Training Info\", f\"{PLAYER_COUNT} player model\")\n",
    "    mlflow.log_param(\"num_players\", PLAYER_COUNT)\n",
    "    mlflow.log_param(\"num_training_steps\", NUM_TRAINING_STEPS)\n",
    "    mlflow.log_param(\"num_eval_games\", NUM_EVAL_GAMES)\n",
    "    mlflow.log_params(MODEL_HYPERPARAMS)\n",
    "\n",
    "    training_result, model_path = train(steps=NUM_TRAINING_STEPS)\n",
    "    onnx_model, onnx_file = onnx_export(model_path)\n",
    "\n",
    "    baseline_results = eval_action_mask(-1, None, num_games=NUM_EVAL_GAMES)\n",
    "    results_by_position = []\n",
    "    for position in range(PLAYER_COUNT):\n",
    "        results_by_position.append(eval_action_mask(position, model_path, num_games=NUM_EVAL_GAMES))\n",
    "\n",
    "    fig = make_plot([baseline_results], \"All random policy\")\n",
    "    mlflow.log_figure(fig, \"basline_random_policy.png\")\n",
    "    fig = make_plot(results_by_position, \"Winrates by Model player position\")\n",
    "    mlflow.log_figure(fig, \"winrates_by_position.png\")\n",
    "\n",
    "    for i, result in enumerate(baseline_results):\n",
    "            mlflow.log_metric(\n",
    "                f\"baseline_winrate_player_{i}\", baseline_results[i][\"winrate\"]\n",
    "            )\n",
    "            mlflow.log_metric(\n",
    "                f\"baseline_total_rewards_player_{i}\", baseline_results[i][\"total_rewards\"]\n",
    "            )\n",
    "\n",
    "    for i, result in enumerate(results_by_position):\n",
    "        for j, player in enumerate(result):\n",
    "            mlflow.log_metric(f\"model_pos_{i}_winrate_player_{j}\", player[\"winrate\"])\n",
    "            mlflow.log_metric(f\"model_pos_{i}_total_rewards_player_{j}\", player[\"total_rewards\"])\n",
    "\n",
    "    example_input = game_embedding(BriscolaGame(players=PLAYER_COUNT), 0)\n",
    "    sig = mlflow.models.infer_signature(example_input)\n",
    "    mlflow.pytorch.log_model(\n",
    "        training_result.policy,\n",
    "        model_path,\n",
    "        signature=sig,\n",
    "        registered_model_name=\"briscola\",\n",
    "    )\n",
    "    mlflow.onnx.log_model(\n",
    "        onnx_model,\n",
    "        onnx_file,\n",
    "        signature=sig,\n",
    "        registered_model_name=\"briscola_onnx\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
