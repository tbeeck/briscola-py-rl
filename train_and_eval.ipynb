{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pettingzoo.test import api_test\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from lib.action_mask_wrapper import SB3ActionMaskWrapper\n",
    "from lib.briscola_env.briscola_env import BriscolaEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYER_COUNT = 2\n",
    "NUM_TRAINING_STEPS = 100_000\n",
    "NUM_EVAL_GAMES = 100\n",
    "\n",
    "\n",
    "def make_env():\n",
    "\treturn BriscolaEnv(num_players=PLAYER_COUNT)\n",
    "\n",
    "def make_model(env):\n",
    "\treturn MaskablePPO(\n",
    "        MaskableActorCriticPolicy, \n",
    "        env, \n",
    "        verbose=1,\n",
    "        learning_rate=3e-4,\n",
    "        ent_coef=0.001,\n",
    "        vf_coef=0.65, \n",
    "        normalize_advantage=True\n",
    "    )\n",
    "\n",
    "api_test(make_env(), num_cycles=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    return env.action_mask()\n",
    "\n",
    "def train(\n",
    "    steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = make_env()\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "    env.reset(seed=seed)\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata)}.\")\n",
    "    model = make_model(env)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps)\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(steps=NUM_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Code\n",
    "\n",
    "def make_results(agents, wins, total_rewards):\n",
    "    player_results = []\n",
    "    total_wins = sum(wins.values())\n",
    "    print(\"Winrates:\")\n",
    "    for p in agents:\n",
    "        winrate = wins[p] / total_wins\n",
    "        print(f\"\\t{p}: {winrate*100}%\")\n",
    "        print(f\"\\tWins: {wins[p]} Rewards: {total_rewards[p]}\")\n",
    "        player_results.append({\"winrate\": winrate, \"total_rewards\": total_rewards[p]})\n",
    "    return player_results\n",
    "\n",
    "def eval_action_mask(player, num_games=100):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = make_env()\n",
    "    print(\"Starting evaluation vs random agents.\")\n",
    "    if player != -1:\n",
    "        print(f\"Trained agent will play as {env.possible_agents[player]}\")\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        raise\n",
    "    print(\"Using policy:\", latest_policy)\n",
    "    model = MaskablePPO.load(latest_policy)\n",
    "\n",
    "    wins = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            observation, action_mask = obs.values()\n",
    "            if termination or truncation:\n",
    "                winner = max(env.rewards, key=env.rewards.get)\n",
    "                wins[winner] += env.infos[winner][\"wins\"] \n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                break\n",
    "            else:\n",
    "                # Act randomly unless it's the agents turn\n",
    "                if player == -1 or agent != env.possible_agents[player]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "    return make_results(env.agents, wins, total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluations\n",
    "baseline_results = eval_action_mask(-1, num_games=NUM_EVAL_GAMES)\n",
    "results_by_position = []\n",
    "for position in range(PLAYER_COUNT):\n",
    "\tresults_by_position.append(eval_action_mask(position, num_games=NUM_EVAL_GAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Code\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def make_plot(winrate_data, title):\n",
    "\twinrates = np.transpose(np.array([\n",
    "\t\t[entry['winrate'] for entry in sublist] \n",
    "\t\tfor sublist in winrate_data\n",
    "   \t]))\n",
    "\tfig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\t# How many groups and bars\n",
    "\tnum_positions, num_players = winrates.shape\n",
    "\tx = np.arange(num_players)\n",
    "\n",
    "\t# Width of each bar group\n",
    "\tbar_width = 0.2\n",
    "\t\n",
    "\t# Plot each group's bars\n",
    "\tfor i in range(num_positions):\n",
    "\t\tax.bar(x + i * bar_width, winrates[i], width=bar_width, label=f'Player {i}')\n",
    "\n",
    "\t# Labels and stuff\n",
    "\tax.set_xlabel('Model Position')\n",
    "\tax.set_ylabel('Winrate')\n",
    "\tax.set_title(title)\n",
    "\tax.set_xticks(x + bar_width * (num_positions-1) / 2)\n",
    "\tax.set_xticklabels([str(i) for i in range(num_players)])\n",
    "\tax.legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\tplt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate diagrams\n",
    "make_plot([baseline_results], \"All random policy\")\n",
    "make_plot(results_by_position, \"Winrates by Model player position\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
