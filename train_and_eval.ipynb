{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msb3_contrib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmaskable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolicies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaskableActorCriticPolicy\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msb3_contrib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ActionMasker\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbriscola_env\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbriscola_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BriscolaEnv\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pettingzoo.test import api_test\n",
    "import pettingzoo\n",
    "import gymnasium as gym\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from lib.briscola_env.briscola_env import BriscolaEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API test\n",
      "Passed API test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:242: UserWarning: Observation space for each agent probably should be gymnasium.spaces.box or gymnasium.spaces.discrete\n",
      "  warnings.warn(\n",
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:140: UserWarning: Observation is not a NumPy array\n",
      "  warnings.warn(\"Observation is not a NumPy array\")\n",
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:660: UserWarning: Environment has not defined a render() method\n",
      "  warnings.warn(\"Environment has not defined a render() method\")\n"
     ]
    }
   ],
   "source": [
    "env = BriscolaEnv()\n",
    "api_test(env, num_cycles=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pass into other gymnasium wrappers, we need to ensure that pettingzoo's wrappper\n",
    "# can also be a gymnasium Env. Thus, we subclass under gym.Env as well.\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper, gym.Env):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\n",
    "\n",
    "        The observation is for the next agent (used to determine the next action), while the remaining\n",
    "        items are for the agent that just acted (used to understand what just happened).\n",
    "        \"\"\"\n",
    "        current_agent = self.agent_selection\n",
    "\n",
    "        super().step(action)\n",
    "\n",
    "        next_agent = self.agent_selection\n",
    "        return (\n",
    "            self.observe(next_agent),\n",
    "            self._cumulative_rewards[current_agent],\n",
    "            self.terminations[current_agent],\n",
    "            self.truncations[current_agent],\n",
    "            self.infos[current_agent],\n",
    "        )\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    return env.action_mask()\n",
    "\n",
    "def train(\n",
    "    steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = BriscolaEnv()\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "    env.reset(seed=seed)\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata)}.\")\n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps)\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on {'render_modes': []}.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 40       |\n",
      "|    ep_rew_mean     | 609      |\n",
      "| time/              |          |\n",
      "|    fps             | 325      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Model has been saved.\n",
      "Finished training on briscola.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(num_games = 100):\n",
    "    env = BriscolaEnv()\n",
    "    print(\"Starting baseline evalutation using all random agents\")\n",
    "    wins = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "            if termination or truncation:\n",
    "                winner = max(env.rewards, key=env.rewards.get)\n",
    "                wins[winner] += env.infos[\n",
    "                    winner\n",
    "                ][\"wins\"] # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                break\n",
    "            else:\n",
    "                act = env.action_space(agent).sample(action_mask)\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    player_results = []\n",
    "    print(\"Winrates:\")\n",
    "    for p in env.agents:\n",
    "        if sum(wins.values()) == 0:\n",
    "            winrate = 0\n",
    "        else:\n",
    "            winrate = wins[p] / sum(wins.values())\n",
    "        print(f\"\\t{p}: {winrate*100}%\")\n",
    "        print(f\"\\tWins: {wins[p]} Rewards: {total_rewards[p]}\")\n",
    "        player_results.append({\"winrate\": winrate})\n",
    "    return player_results\n",
    "\n",
    "\n",
    "def eval_action_mask(player, num_games=100):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = BriscolaEnv()\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs random agents. Trained agent will play as {env.possible_agents[player]}.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "    print(\"using\", latest_policy)\n",
    "    model = MaskablePPO.load(latest_policy)\n",
    "\n",
    "    wins = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                winner = max(env.rewards, key=env.rewards.get)\n",
    "                wins[winner] += env.infos[\n",
    "                    winner\n",
    "                ][\"wins\"] # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                break\n",
    "            else:\n",
    "                if agent != env.possible_agents[player]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    player_results = []\n",
    "    print(\"Winrates:\")\n",
    "    for p in env.agents:\n",
    "        if sum(wins.values()) == 0:\n",
    "            winrate = 0\n",
    "        else:\n",
    "            winrate = wins[p] / sum(wins.values())\n",
    "        print(f\"\\t{p}: {winrate*100}%\")\n",
    "        print(f\"\\tWins: {wins[p]} Rewards: {total_rewards[p]}\")\n",
    "        player_results.append({\"winrate\": winrate})\n",
    "    return player_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting baseline evalutation using all random agents\n",
      "Winrates:\n",
      "\tplayer_0: 32.0%`\n",
      "\tWins: 160 Rewards: 54984\n",
      "\tplayer_1: 18.6%`\n",
      "\tWins: 93 Rewards: 37926\n",
      "\tplayer_2: 29.2%`\n",
      "\tWins: 146 Rewards: 52298\n",
      "\tplayer_3: 20.200000000000003%`\n",
      "\tWins: 101 Rewards: 39792\n",
      "--- Testing position 0 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_0.\n",
      "using briscola_20250427-152319.zip\n",
      "Winrates:\n",
      "\tplayer_0: 31.8%`\n",
      "\tWins: 159 Rewards: 54586\n",
      "\tplayer_1: 21.2%`\n",
      "\tWins: 106 Rewards: 42181\n",
      "\tplayer_2: 26.6%`\n",
      "\tWins: 133 Rewards: 48751\n",
      "\tplayer_3: 20.4%`\n",
      "\tWins: 102 Rewards: 39482\n",
      "--- Testing position 1 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_1.\n",
      "using briscola_20250427-152319.zip\n",
      "Winrates:\n",
      "\tplayer_0: 27.0%`\n",
      "\tWins: 135 Rewards: 50310\n",
      "\tplayer_1: 19.0%`\n",
      "\tWins: 95 Rewards: 37687\n",
      "\tplayer_2: 36.199999999999996%`\n",
      "\tWins: 181 Rewards: 58821\n",
      "\tplayer_3: 17.8%`\n",
      "\tWins: 89 Rewards: 38182\n",
      "--- Testing position 2 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_2.\n",
      "using briscola_20250427-152319.zip\n",
      "Winrates:\n",
      "\tplayer_0: 30.2%`\n",
      "\tWins: 151 Rewards: 53474\n",
      "\tplayer_1: 19.2%`\n",
      "\tWins: 96 Rewards: 39193\n",
      "\tplayer_2: 29.599999999999998%`\n",
      "\tWins: 148 Rewards: 51417\n",
      "\tplayer_3: 21.0%`\n",
      "\tWins: 105 Rewards: 40916\n",
      "--- Testing position 3 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_3.\n",
      "using briscola_20250427-152319.zip\n",
      "Winrates:\n",
      "\tplayer_0: 32.6%`\n",
      "\tWins: 163 Rewards: 56134\n",
      "\tplayer_1: 24.2%`\n",
      "\tWins: 121 Rewards: 43457\n",
      "\tplayer_2: 24.8%`\n",
      "\tWins: 124 Rewards: 48016\n",
      "\tplayer_3: 18.4%`\n",
      "\tWins: 92 Rewards: 37393\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for position in range(4):\n",
    "\tprint(f\"--- Testing position {position} ---\")\n",
    "\tresults.append(eval_action_mask(position, num_games=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
