{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from pettingzoo.test import api_test\n",
    "import pettingzoo\n",
    "import gymnasium as gym\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "from lib.briscola_env.briscola_env import BriscolaEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API test\n",
      "Passed API test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:242: UserWarning: Observation space for each agent probably should be gymnasium.spaces.box or gymnasium.spaces.discrete\n",
      "  warnings.warn(\n",
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:140: UserWarning: Observation is not a NumPy array\n",
      "  warnings.warn(\"Observation is not a NumPy array\")\n",
      "/home/tim/src/briscola-py/.venv/lib/python3.11/site-packages/pettingzoo/test/api_test.py:660: UserWarning: Environment has not defined a render() method\n",
      "  warnings.warn(\"Environment has not defined a render() method\")\n"
     ]
    }
   ],
   "source": [
    "env = BriscolaEnv()\n",
    "api_test(env, num_cycles=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pass into other gymnasium wrappers, we need to ensure that pettingzoo's wrappper\n",
    "# can also be a gymnasium Env. Thus, we subclass under gym.Env as well.\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper, gym.Env):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\n",
    "\n",
    "        The observation is for the next agent (used to determine the next action), while the remaining\n",
    "        items are for the agent that just acted (used to understand what just happened).\n",
    "        \"\"\"\n",
    "        current_agent = self.agent_selection\n",
    "\n",
    "        super().step(action)\n",
    "\n",
    "        next_agent = self.agent_selection\n",
    "        return (\n",
    "            self.observe(next_agent),\n",
    "            self._cumulative_rewards[current_agent],\n",
    "            self.terminations[current_agent],\n",
    "            self.truncations[current_agent],\n",
    "            self.infos[current_agent],\n",
    "        )\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    return env.action_mask()\n",
    "\n",
    "def train(\n",
    "    steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = BriscolaEnv()\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "    env.reset(seed=seed)\n",
    "    env = ActionMasker(env, mask_fn)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata)}.\")\n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps)\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on {'render_modes': []}.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 40       |\n",
      "|    ep_rew_mean     | 605      |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Model has been saved.\n",
      "Finished training on briscola.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(num_games = 100):\n",
    "    env = BriscolaEnv()\n",
    "    print(\"Starting baseline evalutation using all random agents\")\n",
    "    wins = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "            if termination or truncation:\n",
    "                winner = max(env.rewards, key=env.rewards.get)\n",
    "                wins[winner] += env.infos[\n",
    "                    winner\n",
    "                ][\"wins\"] # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                break\n",
    "            else:\n",
    "                act = env.action_space(agent).sample(action_mask)\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    player_results = []\n",
    "    print(\"Winrates:\")\n",
    "    for p in env.agents:\n",
    "        if sum(wins.values()) == 0:\n",
    "            winrate = 0\n",
    "        else:\n",
    "            winrate = wins[p] / sum(wins.values())\n",
    "        print(f\"\\t{p}: {winrate*100}%\")\n",
    "        print(f\"\\tWins: {wins[p]} Rewards: {total_rewards[p]}\")\n",
    "        player_results.append({\"winrate\": winrate})\n",
    "    return player_results\n",
    "\n",
    "\n",
    "def eval_action_mask(player, num_games=100):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = BriscolaEnv()\n",
    "    print(\n",
    "        f\"Starting evaluation vs random agents. Trained agent will play as {env.possible_agents[player]}.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "    print(\"using\", latest_policy)\n",
    "    model = MaskablePPO.load(latest_policy)\n",
    "\n",
    "    wins = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                winner = max(env.rewards, key=env.rewards.get)\n",
    "                wins[winner] += env.infos[\n",
    "                    winner\n",
    "                ][\"wins\"] # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                break\n",
    "            else:\n",
    "                if agent != env.possible_agents[player]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    player_results = []\n",
    "    print(\"Winrates:\")\n",
    "    for p in env.agents:\n",
    "        if sum(wins.values()) == 0:\n",
    "            winrate = 0\n",
    "        else:\n",
    "            winrate = wins[p] / sum(wins.values())\n",
    "        print(f\"\\t{p}: {winrate*100}%\")\n",
    "        print(f\"\\tWins: {wins[p]} Rewards: {total_rewards[p]}\")\n",
    "        player_results.append({\"winrate\": winrate})\n",
    "    return player_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing position 0 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_0.\n",
      "using briscola_20250427-153905.zip\n",
      "Winrates:\n",
      "\tplayer_0: 30.4%\n",
      "\tWins: 152 Rewards: 53683\n",
      "\tplayer_1: 19.400000000000002%\n",
      "\tWins: 97 Rewards: 38123\n",
      "\tplayer_2: 25.8%\n",
      "\tWins: 129 Rewards: 48340\n",
      "\tplayer_3: 24.4%\n",
      "\tWins: 122 Rewards: 44854\n",
      "--- Testing position 1 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_1.\n",
      "using briscola_20250427-153905.zip\n",
      "Winrates:\n",
      "\tplayer_0: 31.0%\n",
      "\tWins: 155 Rewards: 53945\n",
      "\tplayer_1: 22.0%\n",
      "\tWins: 110 Rewards: 42268\n",
      "\tplayer_2: 26.8%\n",
      "\tWins: 134 Rewards: 48469\n",
      "\tplayer_3: 20.200000000000003%\n",
      "\tWins: 101 Rewards: 40318\n",
      "--- Testing position 2 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_2.\n",
      "using briscola_20250427-153905.zip\n",
      "Winrates:\n",
      "\tplayer_0: 31.0%\n",
      "\tWins: 155 Rewards: 54304\n",
      "\tplayer_1: 20.200000000000003%\n",
      "\tWins: 101 Rewards: 38938\n",
      "\tplayer_2: 27.0%\n",
      "\tWins: 135 Rewards: 50160\n",
      "\tplayer_3: 21.8%\n",
      "\tWins: 109 Rewards: 41598\n",
      "--- Testing position 3 ---\n",
      "Starting evaluation vs random agents. Trained agent will play as player_3.\n",
      "using briscola_20250427-153905.zip\n",
      "Winrates:\n",
      "\tplayer_0: 29.799999999999997%\n",
      "\tWins: 149 Rewards: 52286\n",
      "\tplayer_1: 23.0%\n",
      "\tWins: 115 Rewards: 42638\n",
      "\tplayer_2: 27.200000000000003%\n",
      "\tWins: 136 Rewards: 51257\n",
      "\tplayer_3: 20.0%\n",
      "\tWins: 100 Rewards: 38819\n"
     ]
    }
   ],
   "source": [
    "NUM_EVAL_GAMES = 500\n",
    "\n",
    "results_by_position = []\n",
    "for position in range(4):\n",
    "\tprint(f\"--- Testing position {position} ---\")\n",
    "\tresults_by_position.append(eval_action_mask(position, num_games=NUM_EVAL_GAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting baseline evalutation using all random agents\n",
      "Winrates:\n",
      "\tplayer_0: 32.6%\n",
      "\tWins: 163 Rewards: 56334\n",
      "\tplayer_1: 19.400000000000002%\n",
      "\tWins: 97 Rewards: 38143\n",
      "\tplayer_2: 25.4%\n",
      "\tWins: 127 Rewards: 47733\n",
      "\tplayer_3: 22.6%\n",
      "\tWins: 113 Rewards: 42790\n",
      "[[{'winrate': 0.304}, {'winrate': 0.194}, {'winrate': 0.258}, {'winrate': 0.244}], [{'winrate': 0.31}, {'winrate': 0.22}, {'winrate': 0.268}, {'winrate': 0.202}], [{'winrate': 0.31}, {'winrate': 0.202}, {'winrate': 0.27}, {'winrate': 0.218}], [{'winrate': 0.298}, {'winrate': 0.23}, {'winrate': 0.272}, {'winrate': 0.2}]]\n"
     ]
    }
   ],
   "source": [
    "baseline_results = baseline(NUM_EVAL_GAMES)\n",
    "\n",
    "print(results_by_position)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
